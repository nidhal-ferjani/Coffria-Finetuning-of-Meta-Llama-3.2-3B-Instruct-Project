README.txt

# Affinage Modulaire du Mod√®le Llama 3.2 3B pour Coffria

## üìå Introduction

Ce projet fournit un script Python modulaire con√ßu pour affiner le mod√®le pr√©-entra√Æn√© Llama 3.2 3B. L'objectif principal est d'am√©liorer la capacit√© du mod√®le √† traiter des dialogues en fran√ßais, sp√©cifiquement pour les t√¢ches de recherche documentaire sur la plateforme Coffria. Coffria est une plateforme d√©di√©e √† la recherche s√©curis√©e de documents professionnels. Ce script facilite l'adaptation du mod√®le pour g√©rer des requ√™tes et des r√©sultats li√©s √† des documents acad√©miques, des √©tudes de march√© et des rapports, optimisant ainsi son utilisation dans un contexte professionnel francophone.

## ‚öôÔ∏è Pr√©requis Techniques

Avant de commencer, assurez-vous que votre environnement dispose des pr√©requis suivants :

- Python 3.10 ou sup√©rieur
- Pip (gestionnaire de paquets Python)
- GPU NVIDIA compatible CUDA (recommand√© pour l'affinage, CPU possible pour des tests limit√©s)
- Librairies Python list√©es dans le fichier `requirements.txt` (voir ci-dessous)

Fichier `requirements.txt`:

```text
torch --index-url https://download.pytorch.org/whl/cu118
torchvision --index-url https://download.pytorch.org/whl/cu118
torchaudio --index-url https://download.pytorch.org/whl/cu118
xformers[torch2]
unsloth[colab] @ git+https://github.com/unslothai/unsloth.git
git+https://github.com/huggingface/transformers.git
trl
boto3
pydrive
google-api-python-client
huggingface_hub
-e .

üöÄ Installation et Utilisation
1. Cloner le d√©p√¥t GitHub :

git clone [URL_DU_D√âP√îT]
cd [NOM_DU_R√âPERTOIRE_DU_D√âP√îT]

2. Cr√©er et activer un environnement virtuel (recommand√©) :

python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate sur Windows

3. Installer les d√©pendances :

pip install -r requirements.txt

4. Configuration :

Fichiers YAML : Le script utilise des fichiers YAML pour la configuration. Assurez-vous que les fichiers model_loading_params.yaml, lora_params.yaml et trainer_params.yaml sont correctement configur√©s (voir section "üîß Configuration").

Dataset : Pr√©parez votre jeu de donn√©es de dialogues en fran√ßais et anglais au format JSON, ou modifiez le script pour utiliser un dataset Hugging Face existant. Modifiez le chemin vers votre dataset dans trainer_params.yaml ou directement dans le script principal (fine_tuning.py) si n√©cessaire.

AWS (Optionnel) : Si vous souhaitez t√©l√©charger le mod√®le vers AWS S3, configurez vos identifiants AWS dans aws_config.yaml.

5. Lancer l'affinage :

Ex√©cutez le script principal fine_tuning.py :

python -m src.finetuning.fine_tuning

Le script chargera le mod√®le, pr√©parera le dataset, lancera l'affinage, sauvegardera le mod√®le affin√© et (optionnellement) le t√©l√©chargera vers Hugging Face Hub et S3, selon votre configuration.

üîß Guide de Configuration
La configuration du projet est g√©r√©e via des fichiers YAML pour une modularit√© accrue :

- model_loading_params.yaml : Param√®tres de Chargement du Mod√®le

model_name: "unsloth/Llama-3.2-3B-Instruct"  # Nom du mod√®le Hugging Face
quantization_bits: 4                     # Bits pour la quantification (4 ou 8)
device: "cuda"                           # 'cuda' ou 'cpu'
max_seq_length: 2048                     # Longueur maximale des s√©quences

- lora_params.yaml : Param√®tres LoRA (Low-Rank Adaptation)

r: 16                # Rang LoRA
lora_alpha: 32       # Alpha LoRA
lora_dropout: 0.05   # Dropout LoRA
target_modules:      # Modules cibles pour LoRA
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

- trainer_params.yaml : Param√®tres de l'Entra√Æneur (Trainer)  

dataset_path: "path/dialogues_dataset.json"    # Chemin vers votre dataset JSON
output_dir: "llama-3b-coffria-french"          # R√©pertoire de sortie pour le mod√®le affin√©
per_device_train_batch_size: 4                 # Batch size par GPU
gradient_accumulation_steps: 4                 # √âtapes d'accumulation de gradient
warmup_steps: 20                               # √âtapes de warmup
max_steps: 300                                 # Nombre maximal d'√©tapes d'entra√Ænement
learning_rate: 0.00015                         # Taux d'apprentissage
weight_decay: 0.01                             # D√©croissance du poids
logging_steps: 50                              # Fr√©quence des logs
num_epochs: 3                                  # Nombre d'√©poques
dataset_num_proc: 2                            # Nombre de processus pour le dataset

Modifiez ces fichiers YAML pour adapter la configuration √† vos besoins sp√©cifiques (mod√®le, dataset, hyperparam√®tres, chemins, identifiants AWS).

üõ† Maintenance et Am√©liorations Possibles
Ce script modulaire fournit une base solide pour l'affinage du mod√®le Llama 3.2 3B. Voici quelques pistes d'am√©lioration et de maintenance :

Ajout de m√©triques d'√©valuation : Int√©grer des m√©triques d'√©valuation (perplexity, m√©triques sp√©cifiques √† la t√¢che) pendant et apr√®s l'entra√Ænement pour un suivi plus pr√©cis des performances du mod√®le.

Impl√©mentation d'une boucle d'√©valuation : D√©velopper une fonction d'√©valuation plus compl√®te pour tester le mod√®le affin√© sur un jeu de donn√©es de test d√©di√© et mesurer ses performances sur des t√¢ches sp√©cifiques √† Coffria.

Recherche d'hyperparam√®tres : Mettre en place une recherche d'hyperparam√®tres (via des outils comme Optuna ou Ray Tune) pour optimiser les performances du mod√®le affin√© en explorant diff√©rentes configurations d'hyperparam√®tres LoRA et d'entra√Ænement.

Support pour d'autres mod√®les Llama ou mod√®les de langage : √âtendre le script pour supporter l'affinage d'autres mod√®les de la famille Llama ou d'autres mod√®les de langage pr√©-entra√Æn√©s (en modifiant les classes ModelLoader et potentiellement DatasetPreparation).

Documentation et tests unitaires : Ajouter une documentation plus compl√®te du code (docstrings, commentaires) et d√©velopper des tests unitaires pour assurer la robustesse et la qualit√© du code.

üñä Contribu
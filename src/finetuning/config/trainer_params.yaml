# trainer_params.yaml
traning_params:
  max_seq_length: 2048
  per_device_train_batch_size: 12
  gradient_accumulation_steps: 4
  warmup_steps: 20
  warmup_ratio: 0.1
  max_steps: 300
  learning_rate: 2e-5
  logging_steps: 100
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  seed: 3407
  output_dir: "src/finetuning/llama-store-finetuned/"
  num_epochs: 3
  dataset_num_proc: 2
  report_to: "none"
  save_strategy: "epoch"
  logging_steps: 100
